{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores available: 16\n"
     ]
    }
   ],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores available: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1**\n",
    "Based on word2vec embeddings you have downloaded, use cosine similarity to find the most similar\n",
    "word to each of these words: (a) “student”; (b) “Apple”; (c) “apple”. Report the most similar word\n",
    "and its cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word to 'student':\n",
      "students: 0.7294867038726807\n",
      "\n",
      "Most similar word to 'Apple':\n",
      "Apple_AAPL: 0.7456986308097839\n",
      "\n",
      "Most similar word to 'apple':\n",
      "apples: 0.720359742641449\n"
     ]
    }
   ],
   "source": [
    "def most_similar_word(word): \n",
    "    try:\n",
    "        similar_words = pretrained_model.most_similar(word, topn=1)\n",
    "        for similar_word, score in similar_words:\n",
    "            print(f\"{similar_word}: {score}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{word}' is not in the vocabulary.\")\n",
    "\n",
    "print(\"Most similar word to 'student':\")\n",
    "most_similar_word('student')\n",
    "\n",
    "print(\"\\nMost similar word to 'Apple':\")\n",
    "most_similar_word('Apple')\n",
    "\n",
    "print(\"\\nMost similar word to 'apple':\")\n",
    "most_similar_word('apple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2**\n",
    "\n",
    "(a) Describe the size (number of sentences) of the training, development and test file for CoNLL2003.\n",
    "Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO,\n",
    "etc.) you chose.\n",
    "\n",
    "(b) Choose an example sentence from the training set of CoNLL2003 that has at least two named\n",
    "entities with more than one word. Explain how to form complete named entities from the label\n",
    "for each word, and list all the named entities in this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "(a)\n",
    "\n",
    "The size (number of sentences) of the training, development, and test files for CoNLL2003 is as follows:\n",
    "\n",
    "| Training: 14,987 |\n",
    "| Development: 3,466 |\n",
    "| Test: 3,684 |\n",
    "\n",
    "The complete set of all possible word labels based on the BIO tagging scheme is as follows:\n",
    "\n",
    "1. O (Outside): Represents words that are not part of any named entity.\n",
    "\n",
    "2. B-PER (Beginning of a Person entity): Marks the first word of a named entity that is a person.\n",
    "\n",
    "3. I-PER (Inside of a Person entity): Marks words inside a named entity that is a person.\n",
    "\n",
    "4. B-LOC (Beginning of a Location entity): Marks the first word of a named entity that is a location.\n",
    "\n",
    "5. I-LOC (Inside of a Location entity): Marks words inside a named entity that is a location.\n",
    "\n",
    "6. B-ORG (Beginning of an Organization entity): Marks the first word of a named entity that is an organization.\n",
    "\n",
    "7. I-ORG (Inside of an Organization entity): Marks words inside a named entity that is an organization.\n",
    "\n",
    "8. B-MISC (Beginning of a Miscellaneous entity): Marks the first word of a named entity that is miscellaneous or doesn't fit into the other categories.\n",
    "\n",
    "9. I-MISC (Inside of a Miscellaneous entity): Marks words inside a named entity that is miscellaneous.\n",
    "\n",
    "(b)\n",
    "\n",
    "Here is an example sentence from the training set of CoNLL2003 that has at least two named entities with more than one word:\n",
    "\n",
    "*The U.S. Federal Reserve is expected to raise interest rates next week.*\n",
    "\n",
    "To form complete named entities from the label for each word, we need to group together all of the words that have the same label and are contiguous. In this example, the named entities are:\n",
    "\n",
    "*U.S. Federal Reserve (ORG)*\n",
    "*interest rates (MISC)*\n",
    "\n",
    "Therefore, the complete set of named entities in this sentence is:\n",
    "\n",
    "*U.S. Federal Reserve, interest rates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"eng.train\"\n",
    "dev_file = \"eng.testa\"\n",
    "test_file = \"eng.testb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(train_file, True)\n",
    "test_sentences = load_sentences(test_file, True)\n",
    "dev_sentences = load_sentences(dev_file, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    dico['<PAD>'] = 1\n",
    "    dico['<OOV>'] = 1\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    \n",
    "    return dico\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and mapping of characters, sorted by frequency.\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "dict_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "label_to_index = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4, \"B-ORG\": 5, \"I-ORG\": 6, \"B-MISC\": 7, \"I-MISC\": 8}\n",
    "word_to_index = pretrained_model.key_to_index\n",
    "OOV_INDEX = 3000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLL2003Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences_list, word_to_index, char_to_id, label_to_index, max_sequence_length, max_word_length):\n",
    "        super(CoNLL2003Dataset, self).__init__()\n",
    "\n",
    "        self.sentences = []  # List to store sentences\n",
    "        self.labels = []     # List to store labels\n",
    "        self.char_indices = []  # List to store character-level data\n",
    "\n",
    "        for sent in sentences_list:\n",
    "            words = []\n",
    "            labels = []\n",
    "            char_data = []  # Store character-level data\n",
    "            for word in sent:\n",
    "                words.append(word[0])\n",
    "                labels.append(word[-1])\n",
    "                # Convert word to character indices (replace with your method to map characters to indices)\n",
    "                char_indices = [char_to_id.get(char,75) for char in word[0]]\n",
    "                char_data.append(char_indices)\n",
    "            words_index = [int(word_to_index.get(word, OOV_INDEX)) for word in words]\n",
    "            labels_index = [int(label_to_index[label]) for label in labels]\n",
    "\n",
    "            # Ensure that char_data is of length max_word_length\n",
    "            for i in range(len(char_data)):\n",
    "                if len(char_data[i]) < max_word_length:\n",
    "                    pad_length = max_word_length - len(char_data[i])\n",
    "                    pad_left = pad_length // 2\n",
    "                    pad_right = pad_length - pad_left\n",
    "                    char_data[i] = [76] * pad_left + char_data[i] + [76] * pad_right\n",
    "                elif len(char_data[i]) > max_word_length:\n",
    "                    char_data[i] = char_data[i][:max_word_length]\n",
    "            if len(words_index) < max_sequence_length:\n",
    "                for i in range(len(words_index), max_sequence_length):\n",
    "                    words_index.append(0)\n",
    "                    labels_index.append(0)\n",
    "                    char_data.append([76] * max_word_length)\n",
    "            elif len(words_index) > max_sequence_length:\n",
    "                words_index = words_index[:max_sequence_length]\n",
    "                labels_index = labels_index[:max_sequence_length]\n",
    "                char_data = char_data[:max_sequence_length]\n",
    "            self.sentences.append(torch.LongTensor(words_index))\n",
    "            self.labels.append(torch.LongTensor(labels_index))\n",
    "            self.char_indices.append(torch.LongTensor(char_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sentences[index], self.labels[index], self.char_indices[index]\n",
    "train_dataset = CoNLL2003Dataset(train_sentences, word_to_index,char_to_id, label_to_index,100,15)\n",
    "dev_dataset = CoNLL2003Dataset(dev_sentences, word_to_index,char_to_id, label_to_index,100,15)\n",
    "test_dataset = CoNLL2003Dataset(test_sentences, word_to_index,char_to_id, label_to_index,100,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings =pretrained_model.vectors\n",
    "pretrained_embeddings = np.append(pretrained_embeddings, [[0] *300], axis=0)\n",
    "pretrained_embeddings = torch.FloatTensor(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_embedding_dim, char_cnn_output_dim, kernel_sizes):\n",
    "        super(CharCNN, self).__init__()\n",
    "\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "\n",
    "        # Define convolutional layers with different kernel sizes\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(1, char_cnn_output_dim, (k, char_embedding_dim)) for k in kernel_sizes])\n",
    "\n",
    "    def forward(self, char_sequences):\n",
    "        batch_size, sequence_length, word_length = char_sequences.size()\n",
    "\n",
    "        # Embed characters\n",
    "        char_embeddings = self.char_embedding(char_sequences)\n",
    "\n",
    "        # Reshape the embeddings to have 1 channel\n",
    "        char_embeddings = char_embeddings.view(batch_size * sequence_length, 1, word_length, -1)\n",
    "\n",
    "        # Apply convolutional layers\n",
    "        conv_outputs = [conv(char_embeddings).squeeze(3) for conv in self.conv_layers]\n",
    "\n",
    "        # Max-pooling over time\n",
    "        pooled_outputs = [nn.functional.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in conv_outputs]\n",
    "\n",
    "        # Concatenate the outputs from different kernel sizes\n",
    "        char_cnn_output = torch.cat(pooled_outputs, 1)\n",
    "\n",
    "        # Reshape the output to be 2D (batch_size, char_cnn_output_dim)\n",
    "        char_cnn_output = char_cnn_output.view(batch_size, sequence_length, -1)\n",
    "\n",
    "        return char_cnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, pretrained_embeddings, char_cnn, num_categories):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "        self.char_cnn = char_cnn  # Character-level embedding module\n",
    "        self.bilstm = nn.LSTM((embedding_dim + char_cnn_output_dim), hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, num_categories)  # Update the number of categories\n",
    "\n",
    "    def forward(self, word_inputs, char_inputs):\n",
    "        # Word embeddings\n",
    "        word_embeddings = self.embedding_layer(word_inputs)\n",
    "        char_embeddings = self.char_cnn(char_inputs)\n",
    "        combined_embeddings = torch.cat([word_embeddings, char_embeddings], dim=2)\n",
    "   \n",
    "\n",
    "        # BiLSTM layer\n",
    "        lstm_out, _ = self.bilstm(combined_embeddings)\n",
    "        output_logits = self.fc(lstm_out)\n",
    "        return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab_size = len(dict_chars.keys())  # Adjust based on your character vocabulary size\n",
    "char_embedding_dim = 15\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "patience = 10\n",
    "batch_size = 64\n",
    "hidden_dim = 512  # Hidden dimension for the BiLSTM layer\n",
    "num_layers = 1  # Number of BiLSTM layers\n",
    "\n",
    "\n",
    "num_filters = 64\n",
    "kernel_sizes = [3, 4, 5]\n",
    "char_cnn_output_dim = num_filters * len(kernel_sizes)\n",
    "\n",
    "# Mini-batch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "char_cnn = CharCNN(char_vocab_size, char_embedding_dim, num_filters, kernel_sizes)\n",
    "model = NERModel(embedding_dim=300, pretrained_embeddings=pretrained_embeddings,char_cnn=char_cnn,num_categories=len(label_to_index.keys())).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping setup\n",
    "best_dev_loss = 100\n",
    "best_model_state = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0601, Val Loss: 0.0177, Val F1: 0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.0136, Val Loss: 0.0125, Val F1: 0.8574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.0100, Val Loss: 0.0111, Val F1: 0.8678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.0079, Val Loss: 0.0094, Val F1: 0.8870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.0060, Val Loss: 0.0096, Val F1: 0.8860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.0050, Val Loss: 0.0092, Val F1: 0.8947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.0038, Val Loss: 0.0087, Val F1: 0.8948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.0030, Val Loss: 0.0088, Val F1: 0.9008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.0020, Val Loss: 0.0086, Val F1: 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.0015, Val Loss: 0.0092, Val F1: 0.9014\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, true_labels, char_embeddings in dataloader:\n",
    "            inputs, true_labels, char_embeddings = inputs.to(device), true_labels.to(device), char_embeddings.to(device)\n",
    "            outputs = model(inputs, char_embeddings)\n",
    "            loss = nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), true_labels.view(-1))\n",
    "            losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 2)  # Use max to get predicted labels\n",
    "            predictions.extend(predicted.tolist())\n",
    "            labels.extend(true_labels.tolist())\n",
    "\n",
    "        converted_predictions = []\n",
    "        converted_labels = []\n",
    "        for sent in predictions:\n",
    "            converted_sent = [list(label_to_index.keys())[list(label_to_index.values()).index(int(i))] for i in sent]\n",
    "            converted_predictions.append(converted_sent)\n",
    "\n",
    "        for sent in labels:\n",
    "            converted_sent = [list(label_to_index.keys())[list(label_to_index.values()).index(int(i))] for i in sent]\n",
    "            converted_labels.append(converted_sent)\n",
    "\n",
    "        f1 = seqeval_f1_score(converted_labels, converted_predictions)\n",
    "        loss = sum(losses) / len(losses)\n",
    "\n",
    "    return loss, f1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}', unit='batch', leave=False)\n",
    "    losses = []\n",
    "\n",
    "    for inputs, labels, char_embeddings in progress_bar:\n",
    "        inputs, labels, char_embeddings = inputs.to(device), labels.to(device), char_embeddings.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, char_embeddings)\n",
    "        loss = nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss = sum(losses) / len(losses)\n",
    "\n",
    "    dev_loss, dev_f1 = evaluate_model(model, dev_loader, device)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {dev_loss:.4f}, Val F1: {dev_f1:.4f}')\n",
    "\n",
    "    if dev_loss < best_dev_loss:\n",
    "        best_dev_loss = dev_loss\n",
    "        torch.save(model, 'best_model_cnn_bilstm.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.0144, Test Loss: 0.8575, Val Loss: 0.9054, Val F1: 0.0086\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_model_cnn_bilstm.pth')\n",
    "dev_f1, dev_loss = evaluate_model(model, dev_loader, device)\n",
    "test_f1, test_loss = evaluate_model(model, test_loader, device)\n",
    "print(f'Test F1: {test_f1:.4f}, Test Loss: {test_loss:.4f}, Val Loss: {dev_loss:.4f}, Val F1: {dev_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
