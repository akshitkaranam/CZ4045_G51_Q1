{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores available: 16\n"
     ]
    }
   ],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores available: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"eng.train\"\n",
    "dev_file = \"eng.testa\"\n",
    "test_file = \"eng.testb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(train_file, True)\n",
    "test_sentences = load_sentences(test_file, True)\n",
    "dev_sentences = load_sentences(dev_file, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    dico['<PAD>'] = 1\n",
    "    dico['<OOV>'] = 1\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    \n",
    "    return dico\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and mapping of characters, sorted by frequency.\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "dict_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "label_to_index = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4, \"B-ORG\": 5, \"I-ORG\": 6, \"B-MISC\": 7, \"I-MISC\": 8}\n",
    "word_to_index = pretrained_model.key_to_index\n",
    "OOV_INDEX = 3000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLL2003Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences_list, word_to_index, char_to_id, label_to_index, max_sequence_length, max_word_length):\n",
    "        super(CoNLL2003Dataset, self).__init__()\n",
    "\n",
    "        self.sentences = []  # List to store sentences\n",
    "        self.labels = []     # List to store labels\n",
    "        self.char_indices = []  # List to store character-level data\n",
    "\n",
    "        for sent in sentences_list:\n",
    "            words = []\n",
    "            labels = []\n",
    "            char_data = []  # Store character-level data\n",
    "            for word in sent:\n",
    "                words.append(word[0])\n",
    "                labels.append(word[-1])\n",
    "                # Convert word to character indices (replace with your method to map characters to indices)\n",
    "                char_indices = [char_to_id.get(char,75) for char in word[0]]\n",
    "                char_data.append(char_indices)\n",
    "            words_index = [int(word_to_index.get(word, OOV_INDEX)) for word in words]\n",
    "            labels_index = [int(label_to_index[label]) for label in labels]\n",
    "\n",
    "            # Ensure that char_data is of length max_word_length\n",
    "            for i in range(len(char_data)):\n",
    "                if len(char_data[i]) < max_word_length:\n",
    "                    pad_length = max_word_length - len(char_data[i])\n",
    "                    pad_left = pad_length // 2\n",
    "                    pad_right = pad_length - pad_left\n",
    "                    char_data[i] = [76] * pad_left + char_data[i] + [76] * pad_right\n",
    "                elif len(char_data[i]) > max_word_length:\n",
    "                    char_data[i] = char_data[i][:max_word_length]\n",
    "            if len(words_index) < max_sequence_length:\n",
    "                for i in range(len(words_index), max_sequence_length):\n",
    "                    words_index.append(0)\n",
    "                    labels_index.append(0)\n",
    "                    char_data.append([76] * max_word_length)\n",
    "            elif len(words_index) > max_sequence_length:\n",
    "                words_index = words_index[:max_sequence_length]\n",
    "                labels_index = labels_index[:max_sequence_length]\n",
    "                char_data = char_data[:max_sequence_length]\n",
    "            self.sentences.append(torch.LongTensor(words_index))\n",
    "            self.labels.append(torch.LongTensor(labels_index))\n",
    "            self.char_indices.append(torch.LongTensor(char_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sentences[index], self.labels[index], self.char_indices[index]\n",
    "train_dataset = CoNLL2003Dataset(train_sentences, word_to_index,char_to_id, label_to_index,100,15)\n",
    "dev_dataset = CoNLL2003Dataset(dev_sentences, word_to_index,char_to_id, label_to_index,100,15)\n",
    "test_dataset = CoNLL2003Dataset(test_sentences, word_to_index,char_to_id, label_to_index,100,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings =pretrained_model.vectors\n",
    "pretrained_embeddings = np.append(pretrained_embeddings, [[0] *300], axis=0)\n",
    "pretrained_embeddings = torch.FloatTensor(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_embedding_dim, char_cnn_output_dim, kernel_sizes):\n",
    "        super(CharCNN, self).__init__()\n",
    "\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "\n",
    "        # Define convolutional layers with different kernel sizes\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(1, char_cnn_output_dim, (k, char_embedding_dim)) for k in kernel_sizes])\n",
    "\n",
    "    def forward(self, char_sequences):\n",
    "        batch_size, sequence_length, word_length = char_sequences.size()\n",
    "\n",
    "        # Embed characters\n",
    "        char_embeddings = self.char_embedding(char_sequences)\n",
    "\n",
    "        # Reshape the embeddings to have 1 channel\n",
    "        char_embeddings = char_embeddings.view(batch_size * sequence_length, 1, word_length, -1)\n",
    "\n",
    "        # Apply convolutional layers\n",
    "        conv_outputs = [conv(char_embeddings).squeeze(3) for conv in self.conv_layers]\n",
    "\n",
    "        # Max-pooling over time\n",
    "        pooled_outputs = [nn.functional.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in conv_outputs]\n",
    "\n",
    "        # Concatenate the outputs from different kernel sizes\n",
    "        char_cnn_output = torch.cat(pooled_outputs, 1)\n",
    "\n",
    "        # Reshape the output to be 2D (batch_size, char_cnn_output_dim)\n",
    "        char_cnn_output = char_cnn_output.view(batch_size, sequence_length, -1)\n",
    "\n",
    "        return char_cnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NERModel class\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, final_vector_dim, pretrained_embeddings):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, final_vector_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding_layer(inputs)\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        final_vector_representations = self.fc(lstm_out)\n",
    "\n",
    "        return final_vector_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab_size = len(dict_chars.keys())  # Adjust based on your character vocabulary size\n",
    "char_embedding_dim = 15\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "patience = 10\n",
    "batch_size = 64\n",
    "hidden_dim = 512  # Hidden dimension for the BiLSTM layer\n",
    "num_layers = 1  # Number of BiLSTM layers\n",
    "\n",
    "\n",
    "num_filters = 64\n",
    "kernel_sizes = [3, 4, 5]\n",
    "char_cnn_output_dim = num_filters * len(kernel_sizes)\n",
    "\n",
    "# Mini-batch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "char_cnn = CharCNN(char_vocab_size, char_embedding_dim, num_filters, kernel_sizes)\n",
    "model = NERModel(embedding_dim=300, pretrained_embeddings=pretrained_embeddings,final_vector_dim=len(label_to_index.keys())).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping setup\n",
    "best_dev_loss = 100\n",
    "best_model_state = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.1081, Val Loss: 0.0251, Val F1: 0.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.0195, Val Loss: 0.0191, Val F1: 0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.0155, Val Loss: 0.0162, Val F1: 0.8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.0131, Val Loss: 0.0139, Val F1: 0.8505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.0113, Val Loss: 0.0136, Val F1: 0.8542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.0100, Val Loss: 0.0126, Val F1: 0.8698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.0088, Val Loss: 0.0126, Val F1: 0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.0078, Val Loss: 0.0123, Val F1: 0.8769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.0068, Val Loss: 0.0123, Val F1: 0.8792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.0056, Val Loss: 0.0128, Val F1: 0.8753\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, true_labels, char_embeddings in dataloader:\n",
    "            inputs, true_labels, char_embeddings = inputs.to(device), true_labels.to(device), char_embeddings.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), true_labels.view(-1))\n",
    "            losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 2)  # Use max to get predicted labels\n",
    "            predictions.extend(predicted.tolist())\n",
    "            labels.extend(true_labels.tolist())\n",
    "\n",
    "        converted_predictions = []\n",
    "        converted_labels = []\n",
    "        for sent in predictions:\n",
    "            converted_sent = [list(label_to_index.keys())[list(label_to_index.values()).index(int(i))] for i in sent]\n",
    "            converted_predictions.append(converted_sent)\n",
    "\n",
    "        for sent in labels:\n",
    "            converted_sent = [list(label_to_index.keys())[list(label_to_index.values()).index(int(i))] for i in sent]\n",
    "            converted_labels.append(converted_sent)\n",
    "\n",
    "        f1 = seqeval_f1_score(converted_labels, converted_predictions)\n",
    "        loss = sum(losses) / len(losses)\n",
    "\n",
    "    return loss, f1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}', unit='batch', leave=False)\n",
    "    losses = []\n",
    "\n",
    "    for inputs, labels, char_embeddings in progress_bar:\n",
    "        inputs, labels, char_embeddings = inputs.to(device), labels.to(device), char_embeddings.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_loss = sum(losses) / len(losses)\n",
    "\n",
    "    dev_loss, dev_f1 = evaluate_model(model, dev_loader, device)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {dev_loss:.4f}, Val F1: {dev_f1:.4f}')\n",
    "\n",
    "    if dev_loss < best_dev_loss:\n",
    "        best_dev_loss = dev_loss\n",
    "        torch.save(model, 'best_model_cnn_bilstm.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.0169, Test Loss: 0.8202, Val Loss: 0.8769, Val F1: 0.0123\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_model_cnn_bilstm.pth')\n",
    "dev_f1, dev_loss = evaluate_model(model, dev_loader, device)\n",
    "test_f1, test_loss = evaluate_model(model, test_loader, device)\n",
    "print(f'Test F1: {test_f1:.4f}, Test Loss: {test_loss:.4f}, Val Loss: {dev_loss:.4f}, Val F1: {dev_f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
