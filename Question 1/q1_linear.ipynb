{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import multiprocessing\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores available: 16\n"
     ]
    }
   ],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores available: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"eng.train\"\n",
    "dev_file = \"eng.testa\"\n",
    "test_file = \"eng.testb\"\n",
    "\n",
    "label_to_index = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4, \"B-ORG\": 5, \"I-ORG\": 6, \"B-MISC\": 7, \"I-MISC\": 8}\n",
    "index_to_label = { 0: \"O\", 1:\"B-PER\", 2:\"I-PER\", 3:\"B-LOC\", 4:\"I-LOC\", 5:\"B-ORG\", 6:\"I-ORG\", 7:\"B-MISC\", 8:\"I-MISC\"}\n",
    "word_to_index = pretrained_model.key_to_index\n",
    "OOV_INDEX = 3000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoNLL2003Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename, word_to_index, label_to_index, max_sequence_length):\n",
    "        super(CoNLL2003Dataset, self).__init__()\n",
    "\n",
    "        self.sentences = []  # List to store sentences\n",
    "        self.labels = []     # List to store labels\n",
    "\n",
    "        with open(filename, \"r\") as f:\n",
    "            sentence = []\n",
    "            labels_new = []\n",
    "            for line in f:\n",
    "                if line == \"\\n\":\n",
    "                    if sentence:  # Ignore empty lines\n",
    "                        words = [word_to_index.get(word,OOV_INDEX) for word in sentence]\n",
    "                        labels_index = [label_to_index[label] for label in labels_new]\n",
    "\n",
    "                    # Applying Padding to ensure same input length\n",
    "                        if len(words) < max_sequence_length:\n",
    "                            for i in range(len(words), max_sequence_length):\n",
    "                                words.append(0)\n",
    "                                labels_index.append(0)\n",
    "                        elif len(words) > max_sequence_length:\n",
    "                            words = words[:max_sequence_length]\n",
    "                            labels_index = labels_index[:max_sequence_length]\n",
    "\n",
    "                        self.sentences.append(torch.LongTensor(np.array(words)))\n",
    "                        self.labels.append(torch.LongTensor(np.array(labels_index)))\n",
    "                    sentence = []\n",
    "                    labels_new = []\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) > 1:\n",
    "                        word, label = parts[0], parts[3]  # Assuming CoNLL-2003 format\n",
    "                        sentence.append(word)\n",
    "                        labels_new.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sentences[index], self.labels[index]\n",
    "\n",
    "train_dataset = CoNLL2003Dataset(train_file, word_to_index, label_to_index,100)\n",
    "dev_dataset = CoNLL2003Dataset(dev_file, word_to_index, label_to_index,100)\n",
    "test_dataset = CoNLL2003Dataset(test_file, word_to_index, label_to_index,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings =pretrained_model.vectors\n",
    "pretrained_embeddings = np.append(pretrained_embeddings, [[0] *300], axis=0)\n",
    "pretrained_embeddings = torch.FloatTensor(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Convert sequences to tensors\n",
    "    inputs, labels = zip(*[(torch.LongTensor(input_seq), torch.LongTensor(label_seq)) for input_seq, label_seq in batch])\n",
    "    inputs = rnn_utils.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    labels = rnn_utils.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    return inputs, labels\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Mini-batch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NERModel class\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, final_vector_dim, pretrained_embeddings):\n",
    "        super(NERModel, self).__init__()\n",
    "\n",
    "        # Create an embedding layer using the pre-trained Word2Vec embeddings.\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # Create a linear layer.\n",
    "        self.linear_layer = nn.Linear(embedding_dim, final_vector_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get the word embeddings.\n",
    "        embeddings = self.embedding_layer(inputs)\n",
    "\n",
    "        # Apply the linear layer.\n",
    "        final_vector_representations = self.linear_layer(embeddings)\n",
    "\n",
    "        return final_vector_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model.\n",
    "model = NERModel(embedding_dim=300, final_vector_dim=300, pretrained_embeddings=pretrained_embeddings).to(device)\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping setup\n",
    "best_dev_f1 = 0.0\n",
    "patience = 3  # Number of epochs without improvement to wait before early stopping\n",
    "\n",
    "# Initialize a variable to track the best model state\n",
    "best_model_state = model.state_dict()\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-LOC',\n",
       " 4: 'I-LOC',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Dev F1: 0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Dev F1: 0.6966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Dev F1: 0.6907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Dev F1: 0.6961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Dev F1: 0.6984\n",
      "Early stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.6451\n",
      "Total running time: 933.38 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Create a tqdm progress bar for the training data\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}', unit='batch', leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Evaluation on the development set\n",
    "    model.eval()\n",
    "    dev_predictions = []\n",
    "    dev_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            dev_predictions.extend(outputs.argmax(dim=2).tolist())\n",
    "            dev_labels.extend(labels.tolist())\n",
    "\n",
    "        converted_dev_predictions = []\n",
    "        converted_dev_labels = []\n",
    "        for sent in dev_predictions:\n",
    "            converted_sent = []\n",
    "            for i in sent:\n",
    "                # Find the corresponding text label for the numerical label 'i'\n",
    "                text_label = list(label_to_index.keys())[list(label_to_index.values()).index(i)]\n",
    "                converted_sent.append(text_label)\n",
    "            converted_dev_predictions.append(converted_sent)\n",
    "\n",
    "        for sent in dev_labels:\n",
    "            converted_sent = []\n",
    "            for i in sent:\n",
    "                # Find the corresponding text label for the numerical label 'i'\n",
    "                text_label = list(label_to_index.keys())[list(label_to_index.values()).index(i)]\n",
    "                converted_sent.append(text_label)\n",
    "            converted_dev_labels.append(converted_sent)   \n",
    "    \n",
    "\n",
    "    dev_f1 = seqeval_f1_score(converted_dev_labels,converted_dev_predictions)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Dev F1: {dev_f1:.4f}')\n",
    "\n",
    "    # Early stopping check\n",
    "    if dev_f1 > best_dev_f1:\n",
    "        best_dev_f1 = dev_f1\n",
    "        # Save the model or its parameters if it improves\n",
    "\n",
    "    # Early stopping condition\n",
    "    if epoch > patience and dev_f1 <= best_dev_f1:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model_state, 'best_model_linear.pth')\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "\n",
    "# Create a tqdm progress bar for the test data\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc='Testing', unit='batch', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_predictions.extend(outputs.argmax(dim=2).tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "\n",
    "converted_test_predictions = []\n",
    "converted_test_labels = []\n",
    "for sent in test_predictions:\n",
    "    converted_sent = []\n",
    "    for i in sent:\n",
    "        # Find the corresponding text label for the numerical label 'i'\n",
    "        text_label = list(label_to_index.keys())[list(label_to_index.values()).index(i)]\n",
    "        converted_sent.append(text_label)\n",
    "    converted_test_predictions.append(converted_sent)\n",
    "\n",
    "for sent in test_labels:\n",
    "    converted_sent = []\n",
    "    for i in sent:\n",
    "        # Find the corresponding text label for the numerical label 'i'\n",
    "        text_label = list(label_to_index.keys())[list(label_to_index.values()).index(i)]\n",
    "        converted_sent.append(text_label)\n",
    "    converted_test_labels.append(converted_sent)   \n",
    "    \n",
    "\n",
    "# test_f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "test_f1 = seqeval_f1_score(converted_test_labels, converted_test_predictions)\n",
    "\n",
    "print(f'Test F1: {test_f1:.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total running time: {end_time - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
